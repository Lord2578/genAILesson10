import time
import json
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class FacebookPropertyScraper:
    def __init__(self, target_url, output_filename="facebook_posts.jsonl"):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∫—Ä–∞–ø–µ—Ä–∞ –¥–ª—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–µ—Ä—É—Ö–æ–º–æ—Å—Ç—ñ"""
        self.target_url = target_url
        self.output_filename = output_filename
        self.driver = None
        self.setup_browser()
        
    def setup_browser(self):
        """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±—Ä–∞—É–∑–µ—Ä–∞ Chrome –¥–ª—è —Å–∫—Ä–∞–ø—ñ–Ω–≥—É"""
        browser_options = Options()
        # browser_options.add_argument("--headless")  # –ó–∞–∫–æ–º–µ–Ω—Ç–æ–≤–∞–Ω–æ –¥–ª—è –Ω–∞–æ—á–Ω–æ—Å—Ç—ñ
        browser_options.add_argument("--disable-notifications")
        browser_options.add_argument("--window-size=1920,1080")
        browser_options.add_argument("--disable-extensions")
        browser_options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        
        self.driver = webdriver.Chrome(options=browser_options)
        self.driver.implicitly_wait(5)
    
    def dismiss_login_popup(self):
        """–ó–∞–∫—Ä–∏—Ç—Ç—è –¥—ñ–∞–ª–æ–≥–æ–≤–æ–≥–æ –≤—ñ–∫–Ω–∞ –≤—Ö–æ–¥—É, —è–∫—â–æ –≤–æ–Ω–æ –∑'—è–≤–∏–ª–æ—Å—è"""
        try:
            print("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –¥—ñ–∞–ª–æ–≥–æ–≤–æ–≥–æ –≤—ñ–∫–Ω–∞ –≤—Ö–æ–¥—É...")
            wait = WebDriverWait(self.driver, 6)
            close_buttons = wait.until(
                EC.any_of(
                    EC.element_to_be_clickable((By.XPATH, "//div[@aria-label='–ó–∞–∫—Ä–∏—Ç–∏']")),
                    EC.element_to_be_clickable((By.XPATH, "//div[@aria-label='Close']"))
                )
            )
            
            close_buttons.click()
            print("‚úÖ –î—ñ–∞–ª–æ–≥–æ–≤–µ –≤—ñ–∫–Ω–æ –≤—Ö–æ–¥—É –∑–∞–∫—Ä–∏—Ç–æ")
            time.sleep(1)
        except (TimeoutException, Exception) as e:
            print(f"‚ÑπÔ∏è –î—ñ–∞–ª–æ–≥–æ–≤–µ –≤—ñ–∫–Ω–æ –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ –∞–±–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–∫—Ä–∏—Ç–∏: {str(e)}")
    
    def load_more_content(self, scroll_count=6):
        """–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
        print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É ({scroll_count} –ø—Ä–æ–∫—Ä—É—Ç–æ–∫)...")
        
        for i in range(scroll_count):
            self.driver.execute_script(
                "window.scrollTo(0, document.documentElement.scrollHeight);"
            )
            print(f"–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ {i+1}/{scroll_count}...")
            time.sleep(2.5)  # –ü–∞—É–∑–∞ –º—ñ–∂ –ø—Ä–æ–∫—Ä—É—Ç–∫–∞–º–∏
    
    def extract_property_posts(self, max_posts=15):
        """–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ –ø–æ—Å—Ç—ñ–≤ –ø—Ä–æ –Ω–µ—Ä—É—Ö–æ–º—ñ—Å—Ç—å"""
        print("–ü–æ—à—É–∫ –ø–æ—Å—Ç—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ...")
        
        # –†—ñ–∑–Ω—ñ XPath-—Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –ø–æ—à—É–∫—É –ø–æ—Å—Ç—ñ–≤
        post_selectors = [
            "//div[contains(@class, 'x1yztbdb')]",
            "//div[contains(@class, 'x1lliihq')]",
            "//div[contains(@class, 'xyamay9') and contains(@class, 'xqcrz7y')]"
        ]
        
        # –°–ø—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏
        post_elements = []
        for selector in post_selectors:
            post_elements = self.driver.find_elements(By.XPATH, selector)
            if post_elements:
                print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(post_elements)} –ø–æ—Å—Ç—ñ–≤ –∑–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {selector}")
                break
        
        if not post_elements:
            print("‚ùó –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –∂–æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞. –ú–æ–∂–ª–∏–≤–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑–º—ñ–Ω–∏–ª–∞—Å—è.")
            return []
        
        # –û–±–º–µ–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –ø–æ—Å—Ç—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏
        post_elements = post_elements[:max_posts]
        
        # –°–ø–∏—Å–æ–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –ø—Ä–æ –ø–æ—Å—Ç–∏
        property_data = []
        
        for idx, post in enumerate(post_elements):
            print(f"–û–±—Ä–æ–±–∫–∞ –ø–æ—Å—Ç–∞ {idx+1}/{len(post_elements)}...")
            
            post_info = {
                "post_url": "URL not found",
                "content": "No content found",
                "date": "Date not found"
            }
            
            # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è URL –ø–æ—Å—Ç–∞
            try:
                # –°–ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑ —Ä—ñ–∑–Ω–∏–º–∏ —à–ª—è—Ö–∞–º–∏
                links = post.find_elements(By.XPATH, ".//a[contains(@href, '/posts/') or contains(@href, '/photos/')]")
                for link in links:
                    href = link.get_attribute('href')
                    if href and ('/posts/' in href or '/photos/' in href):
                        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ URL
                        post_info["post_url"] = href.split('?')[0]
                        break
            except Exception as e:
                print(f"  –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ URL: {str(e)}")
            
            # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –ø–æ—Å—Ç–∞
            try:
                # –°–ø—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –ø–æ—à—É–∫—É —Ç–µ–∫—Å—Ç—É
                text_selectors = [
                    ".//div[@data-ad-comet-preview='message']//div[@dir='auto']",
                    ".//div[contains(@class, 'xdj266r')]//div[@dir='auto']",
                    ".//div[contains(@class, 'x1iorvi4')]//span"
                ]
                
                for selector in text_selectors:
                    text_elements = post.find_elements(By.XPATH, selector)
                    if text_elements:
                        post_info["content"] = text_elements[0].text.strip()
                        break
            except Exception as e:
                print(f"  –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ —Ç–µ–∫—Å—Ç—É: {str(e)}")
            
            # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –¥–∞—Ç–∏ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó
            try:
                # –°–ø—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –ø–æ—à—É–∫—É –¥–∞—Ç–∏
                date_selectors = [
                    ".//span[contains(@class, 'x4k7w5x')]",
                    ".//span[contains(@class, 'xzsf02u')]",
                    ".//a[contains(@class, 'x1i10hfl')]//span[1]"
                ]
                
                for selector in date_selectors:
                    date_elements = post.find_elements(By.XPATH, selector)
                    if date_elements:
                        post_info["date"] = date_elements[0].text.strip()
                        break
            except Exception as e:
                print(f"  –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –¥–∞—Ç–∏: {str(e)}")
            
            # –î–æ–¥–∞—î–º–æ –¥–∞–Ω—ñ –ø–æ—Å—Ç–∞ –¥–æ —Å–ø–∏—Å–∫—É, —è–∫—â–æ —î URL –∞–±–æ —Ç–µ–∫—Å—Ç
            if post_info["post_url"] != "URL not found" or post_info["content"] != "No content found":
                property_data.append(post_info)
                print(f"  ‚úÖ –î–∞–Ω—ñ –ø–æ—Å—Ç–∞ –¥–æ–¥–∞–Ω–æ: {post_info['content'][:30]}...")
        
        return property_data
    
    def save_data_to_file(self, data):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —É JSONL —Ñ–∞–π–ª"""
        if not data:
            print("‚ùó –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è")
            return
        
        try:
            with open(self.output_filename, 'w', encoding='utf-8') as f:
                for item in data:
                    json.dump(item, f, ensure_ascii=False)
                    f.write('\n')
            
            print(f"‚úÖ –î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª: {self.output_filename}")
            print(f"   –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å—ñ–≤")
        except Exception as e:
            print(f"‚ùó –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –¥–∞–Ω–∏—Ö: {str(e)}")
    
    def run(self, max_posts=15):
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—É —Å–∫—Ä–∞–ø—ñ–Ω–≥—É"""
        try:
            print(f"üöÄ –ü–æ—á–∞—Ç–æ–∫ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É: {self.target_url}")
            
            # –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ü—ñ–ª—å–æ–≤—É —Å—Ç–æ—Ä—ñ–Ω–∫—É
            self.driver.get(self.target_url)
            print("‚úÖ –°—Ç–æ—Ä—ñ–Ω–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
            time.sleep(4)  # –ß–µ–∫–∞—î–º–æ –Ω–∞ –ø–æ–≤–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
            
            # –ó–∞–∫—Ä–∏–≤–∞—î–º–æ –ø–æ–ø–∞–ø –≤—Ö–æ–¥—É, —è–∫—â–æ –≤—ñ–Ω –∑'—è–≤–∏–≤—Å—è
            self.dismiss_login_popup()
            
            # –ü—Ä–æ–∫—Ä—É—á—É—î–º–æ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –±—ñ–ª—å—à–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –ø–æ—Å—Ç—ñ–≤
            self.load_more_content()
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –¥–∞–Ω—ñ –∑ –ø–æ—Å—Ç—ñ–≤
            posts_data = self.extract_property_posts(max_posts)
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–∞–Ω—ñ —É —Ñ–∞–π–ª
            self.save_data_to_file(posts_data)
            
            print("üèÅ –°–∫—Ä–∞–ø—ñ–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
            
        except Exception as e:
            print(f"‚ùó –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É: {str(e)}")
        finally:
            if self.driver:
                self.driver.quit()
                print("üîí –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä–∏—Ç–æ")


if __name__ == "__main__":
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –∑–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø–µ—Ä–∞
    TARGET_PAGE = "https://www.facebook.com/providentrealestateuz"
    OUTPUT_FILE = "facebook_posts.jsonl"
    MAX_POSTS = 15
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –∑–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø–µ—Ä–∞
    scraper = FacebookPropertyScraper(TARGET_PAGE, OUTPUT_FILE)
    
    # –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É
    scraper.run(MAX_POSTS)